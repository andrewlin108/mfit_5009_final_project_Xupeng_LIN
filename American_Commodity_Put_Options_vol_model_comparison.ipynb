{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d736960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import brentq, minimize_scalar\n",
    "import time\n",
    "import warnings\n",
    "import QuantLib as ql\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "from scipy.special import erfc\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The risk-free rate is now a parameter that can be adjusted for each method\n",
    "# Default value - will be overridden by parameter versions\n",
    "DEFAULT_R = 0.03\n",
    "\n",
    "\n",
    "# 1. Black-Scholes European approximation with parameters\n",
    "def compute_iv_euro(row, r=DEFAULT_R, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']  # Underlying price\n",
    "        K = row['strike']  # Strike price\n",
    "        T = row['maturity'] / 365  # Time to expiry in years\n",
    "        price = row['mean_mid']  # Option price\n",
    "        is_call = row['is_call'] == 1  # Option type (1=call, 0=put)\n",
    "\n",
    "        # Calculate Black-Scholes implied volatility\n",
    "        def objective(sigma):\n",
    "            return black_scholes_price(S, K, T, r, sigma, is_call) - price\n",
    "\n",
    "        # Use brentq to find the implied volatility with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Black-Scholes pricing formula\n",
    "def black_scholes_price(S, K, T, r, sigma, is_call):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    if is_call:\n",
    "        return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    else:\n",
    "        return K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "\n",
    "\n",
    "# 2. Binomial Tree method with parameters\n",
    "def implied_vol_binomial(row, r=DEFAULT_R, steps=100, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        def objective(sigma):\n",
    "            model_price = binomial_tree_price(S, K, T, r, sigma, is_call, steps=steps)\n",
    "            return model_price - price\n",
    "\n",
    "        # Find the implied volatility using brentq with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Binomial Tree pricing model with parameters\n",
    "def binomial_tree_price(S, K, T, r, sigma, is_call, steps=100):\n",
    "    dt = T / steps\n",
    "    u = np.exp(sigma * np.sqrt(dt))\n",
    "    d = 1 / u\n",
    "    p = (np.exp(r * dt) - d) / (u - d)\n",
    "\n",
    "    # Initialize asset prices at maturity\n",
    "    prices = np.zeros(steps + 1)\n",
    "    for i in range(steps + 1):\n",
    "        prices[i] = S * (u ** (steps - i)) * (d ** i)\n",
    "\n",
    "    # Initialize option values at maturity\n",
    "    if is_call:\n",
    "        values = np.maximum(0, prices - K)\n",
    "    else:\n",
    "        values = np.maximum(0, K - prices)\n",
    "\n",
    "    # Work backwards through the tree\n",
    "    for step in range(steps - 1, -1, -1):\n",
    "        for i in range(step + 1):\n",
    "            # Calculate asset price at this node\n",
    "            asset_price = S * (u ** (step - i)) * (d ** i)\n",
    "\n",
    "            # Calculate option value from children nodes\n",
    "            option_value = np.exp(-r * dt) * (p * values[i] + (1 - p) * values[i + 1])\n",
    "\n",
    "            # Check for early exercise\n",
    "            if is_call:\n",
    "                exercise_value = max(0, asset_price - K)\n",
    "            else:\n",
    "                exercise_value = max(0, K - asset_price)\n",
    "\n",
    "            # Take maximum of continuation and exercise\n",
    "            values[i] = max(option_value, exercise_value)\n",
    "\n",
    "    # Return the option price (root of the tree)\n",
    "    return values[0]\n",
    "\n",
    "\n",
    "import QuantLib as ql\n",
    "\n",
    "\n",
    "def quantlib_binomial_spot_iv(row, r=0.03, steps=200, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    #100 steps is not enough to get normal answer\n",
    "    try:\n",
    "        # Extract option parameters\n",
    "        S = row['current_underlying_mean_mid']  # Use spot price instead of futures price\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        # Setup dates\n",
    "        calculation_date = ql.Date.todaysDate()\n",
    "        ql.Settings.instance().evaluationDate = calculation_date\n",
    "        maturity_date = calculation_date + ql.Period(int(T * 365), ql.Days)\n",
    "\n",
    "        def objective(sigma):\n",
    "            try:\n",
    "                # Create the spot price handle\n",
    "                spot_handle = ql.QuoteHandle(ql.SimpleQuote(S))\n",
    "\n",
    "                # Create yield curves for regular American options\n",
    "                risk_free_ts = ql.YieldTermStructureHandle(\n",
    "                    ql.FlatForward(calculation_date, r, ql.Actual365Fixed())\n",
    "                )\n",
    "                dividend_ts = ql.YieldTermStructureHandle(\n",
    "                    ql.FlatForward(calculation_date, 0.0, ql.Actual365Fixed())  # No dividend\n",
    "                )\n",
    "\n",
    "                # Create volatility surface\n",
    "                vol_ts = ql.BlackVolTermStructureHandle(\n",
    "                    ql.BlackConstantVol(calculation_date, ql.NullCalendar(), sigma, ql.Actual365Fixed())\n",
    "                )\n",
    "\n",
    "                # Create Black-Scholes process for standard options\n",
    "                process = ql.BlackScholesMertonProcess(\n",
    "                    spot_handle,  # Spot price\n",
    "                    dividend_ts,  # Dividend yield\n",
    "                    risk_free_ts,  # Risk-free rate\n",
    "                    vol_ts  # Volatility\n",
    "                )\n",
    "\n",
    "                # Create binomial engine\n",
    "                engine = ql.BinomialVanillaEngine(process, \"crr\", steps)\n",
    "\n",
    "                # Create the option\n",
    "                option_type = ql.Option.Call if is_call else ql.Option.Put\n",
    "                payoff = ql.PlainVanillaPayoff(option_type, K)\n",
    "                exercise = ql.AmericanExercise(calculation_date, maturity_date)\n",
    "                option = ql.VanillaOption(payoff, exercise)\n",
    "\n",
    "                # Price the option\n",
    "                option.setPricingEngine(engine)\n",
    "                model_price = option.NPV()\n",
    "\n",
    "                return model_price - price\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in objective: {e}\")\n",
    "                return 0\n",
    "\n",
    "        # Find the implied volatility\n",
    "        from scipy.optimize import brentq\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# 3. Trinomial Tree method with parameters\n",
    "def implied_vol_trinomial(row, r=DEFAULT_R, steps=100, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        def objective(sigma):\n",
    "            model_price = trinomial_tree_price(S, K, T, r, sigma, is_call, steps=steps)\n",
    "            return model_price - price\n",
    "\n",
    "        # Find the implied volatility using brentq with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Trinomial Tree pricing model with parameters\n",
    "def trinomial_tree_price(S, K, T, r, sigma, is_call, steps=50):\n",
    "    dt = T / steps\n",
    "\n",
    "    # Trinomial tree parameters\n",
    "    u = np.exp(sigma * np.sqrt(2 * dt))\n",
    "    d = 1 / u\n",
    "    m = 1.0\n",
    "\n",
    "    # Risk-neutral probabilities\n",
    "    pu = ((np.exp(r * dt / 2) - np.exp(-sigma * np.sqrt(dt / 2))) /\n",
    "          (np.exp(sigma * np.sqrt(dt / 2)) - np.exp(-sigma * np.sqrt(dt / 2)))) ** 2\n",
    "    pd = ((np.exp(sigma * np.sqrt(dt / 2)) - np.exp(r * dt / 2)) /\n",
    "          (np.exp(sigma * np.sqrt(dt / 2)) - np.exp(-sigma * np.sqrt(dt / 2)))) ** 2\n",
    "    pm = 1 - pu - pd\n",
    "\n",
    "    # Initialize arrays with proper size\n",
    "    max_nodes = 2 * steps + 1\n",
    "    price_tree = np.zeros((steps + 1, max_nodes))\n",
    "    option_tree = np.zeros((steps + 1, max_nodes))\n",
    "\n",
    "    # Build the tree using centered indexing\n",
    "    for i in range(steps + 1):  # Time steps\n",
    "        for j in range(-i, i + 1):  # Node positions at this time step\n",
    "            idx = j + steps  # Convert to array index (center is at position 'steps')\n",
    "            price_tree[i, idx] = S * (u ** j)\n",
    "\n",
    "    # Initialize option values at maturity\n",
    "    for j in range(-steps, steps + 1):\n",
    "        idx = j + steps\n",
    "        if is_call:\n",
    "            option_tree[steps, idx] = max(0, price_tree[steps, idx] - K)\n",
    "        else:\n",
    "            option_tree[steps, idx] = max(0, K - price_tree[steps, idx])\n",
    "\n",
    "    # Work backwards through the tree\n",
    "    for i in range(steps - 1, -1, -1):\n",
    "        for j in range(-i, i + 1):\n",
    "            idx = j + steps\n",
    "\n",
    "            # Get indices for the three children nodes\n",
    "            idx_up = (j + 1) + steps\n",
    "            idx_mid = j + steps\n",
    "            idx_down = (j - 1) + steps\n",
    "\n",
    "            # Calculate expected option value\n",
    "            expected = (pu * option_tree[i + 1, idx_up] +\n",
    "                        pm * option_tree[i + 1, idx_mid] +\n",
    "                        pd * option_tree[i + 1, idx_down]) * np.exp(-r * dt)\n",
    "\n",
    "            # Calculate exercise value\n",
    "            if is_call:\n",
    "                exercise = max(0, price_tree[i, idx] - K)\n",
    "            else:\n",
    "                exercise = max(0, K - price_tree[i, idx])\n",
    "\n",
    "            # Take maximum of continuation and exercise\n",
    "            option_tree[i, idx] = max(expected, exercise)\n",
    "\n",
    "    # Return option price at root (time 0, middle node)\n",
    "    return option_tree[0, steps]\n",
    "\n",
    "# 4. Barone-Adesi and Whaley (BAW) approximation with parameters\n",
    "def baw_american_iv(row, r=DEFAULT_R, q=0.0, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        def objective(sigma):\n",
    "            model_price = american_option_baw(S, K, T, r, sigma, is_call, q=q)\n",
    "            return model_price - price\n",
    "\n",
    "        # Find the implied volatility with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# using quantlib\n",
    "def quantlib_trinomial_iv(row, r=0.03, steps=100, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    \"\"\"Calculate implied volatility using QuantLib's trinomial tree method\"\"\"\n",
    "    try:\n",
    "        # Extract option parameters\n",
    "        S = row['current_underlying_mean_mid']  # Spot price\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        # Setup dates\n",
    "        calculation_date = ql.Date.todaysDate()\n",
    "        ql.Settings.instance().evaluationDate = calculation_date\n",
    "        maturity_date = calculation_date + ql.Period(int(T * 365), ql.Days)\n",
    "\n",
    "        def objective(sigma):\n",
    "            try:\n",
    "                # Create the spot price handle\n",
    "                spot_handle = ql.QuoteHandle(ql.SimpleQuote(S))\n",
    "\n",
    "                # Create yield curves\n",
    "                risk_free_ts = ql.YieldTermStructureHandle(\n",
    "                    ql.FlatForward(calculation_date, r, ql.Actual365Fixed())\n",
    "                )\n",
    "                dividend_ts = ql.YieldTermStructureHandle(\n",
    "                    ql.FlatForward(calculation_date, 0.0, ql.Actual365Fixed())  # No dividend\n",
    "                )\n",
    "\n",
    "                # Create volatility surface\n",
    "                vol_ts = ql.BlackVolTermStructureHandle(\n",
    "                    ql.BlackConstantVol(calculation_date, ql.NullCalendar(), sigma, ql.Actual365Fixed())\n",
    "                )\n",
    "\n",
    "                # Create the process\n",
    "                process = ql.BlackScholesMertonProcess(\n",
    "                    spot_handle, dividend_ts, risk_free_ts, vol_ts\n",
    "                )\n",
    "\n",
    "                # Create trinomial engine - use TrinomialVanillaEngine\n",
    "                engine = ql.TrinomialVanillaEngine(process, steps)\n",
    "\n",
    "                # Create the option\n",
    "                option_type = ql.Option.Call if is_call else ql.Option.Put\n",
    "                payoff = ql.PlainVanillaPayoff(option_type, K)\n",
    "                exercise = ql.AmericanExercise(calculation_date, maturity_date)\n",
    "                option = ql.VanillaOption(payoff, exercise)\n",
    "\n",
    "                # Price the option\n",
    "                option.setPricingEngine(engine)\n",
    "                model_price = option.NPV()\n",
    "\n",
    "                return model_price - price\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in objective: {e}\")\n",
    "                return 0\n",
    "\n",
    "        # Find the implied volatility\n",
    "        from scipy.optimize import brentq\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def american_option_baw(S, K, T, r, sigma, is_call, q=0.0):\n",
    "    \"\"\"Barone-Adesi and Whaley approximation for American options\"\"\"\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    # First calculate European option price\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "\n",
    "    if is_call:\n",
    "        euro_price = S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    else:\n",
    "        euro_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * np.exp(-q * T) * norm.cdf(-d1)\n",
    "\n",
    "    # Early exercise premium\n",
    "    if is_call:\n",
    "        if r <= q:  # No early exercise benefit\n",
    "            return euro_price\n",
    "        else:\n",
    "            # Call option approximation\n",
    "            b = r - q\n",
    "            M = 2 * r / (sigma ** 2)\n",
    "            N = 2 * b / (sigma ** 2)\n",
    "            K_inf = K * N / (N - 1)\n",
    "            q2 = (-(N - 1) + np.sqrt((N - 1) ** 2 + 4 * M / K)) / 2\n",
    "            a2 = (K / q2) * (1 - np.exp(-r * T) * norm.cdf(d2))\n",
    "\n",
    "            if S >= K_inf:\n",
    "                return S - K\n",
    "            else:\n",
    "                return euro_price + a2 * (S / K_inf) ** q2\n",
    "    else:  # Put option\n",
    "        if r >= q:  # Early exercise calculation\n",
    "            b = r - q\n",
    "            M = 2 * r / (sigma ** 2)\n",
    "            N = 2 * b / (sigma ** 2)\n",
    "            K_0 = K * N / (N + 1)\n",
    "            q1 = (-(N - 1) - np.sqrt((N - 1) ** 2 + 4 * M / K)) / 2\n",
    "            a1 = -(K / q1) * (1 - np.exp(-r * T) * norm.cdf(-d2))\n",
    "\n",
    "            if S <= K_0:\n",
    "                return K - S\n",
    "            else:\n",
    "                return euro_price + a1 * (S / K_0) ** q1\n",
    "        else:\n",
    "            return euro_price\n",
    "\n",
    "\n",
    "# 5. Bjerksund-Stensland approximation with parameters\n",
    "def bjerksund_iv(row, r=DEFAULT_R, q=0.0, max_vol=3.0, min_vol=0.001, max_iter=100):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        def objective(sigma):\n",
    "            model_price = american_option_bjerksund(S, K, T, r, sigma, is_call, q=q)\n",
    "            return model_price - price\n",
    "\n",
    "        # Find the implied volatility with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def american_option_bjerksund(S, K, T, r, sigma, is_call, q=0.0):\n",
    "    \"\"\"Simplified Bjerksund-Stensland approximation for American options\"\"\"\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    # For put options, use put-call transformation\n",
    "    if not is_call:\n",
    "        # P(S,K,T,r,σ,q) = C(K,S,T,q,σ,r)\n",
    "        return american_option_bjerksund(K, S, T, q, sigma, True, r)\n",
    "\n",
    "    # Check boundary conditions\n",
    "    if T <= 0:\n",
    "        return max(0, S - K)\n",
    "\n",
    "    # Calculate European price (used as fallback)\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    euro_price = S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "    b = r - q\n",
    "\n",
    "    # Check for early exercise\n",
    "    if b >= r:  # No early exercise for this case\n",
    "        return euro_price\n",
    "\n",
    "    # Calculate parameters for the approximation\n",
    "    beta = (0.5 - b / (sigma ** 2)) + np.sqrt((b / (sigma ** 2) - 0.5) ** 2 + 2 * r / (sigma ** 2))\n",
    "    B_infty = beta * K / (beta - 1)\n",
    "    B_0 = max(K, r * K / b)\n",
    "\n",
    "    h1 = -(b * T + 2 * sigma * np.sqrt(T)) * K / (B_0 - K)\n",
    "    h2 = -(b * T - 2 * sigma * np.sqrt(T)) * K / (B_0 - K)\n",
    "\n",
    "    I1 = B_0 - (B_0 - K) * np.exp(h1)\n",
    "    I2 = B_0 - (B_0 - K) * np.exp(h2)\n",
    "\n",
    "    alpha = (I1 - I2) / (B_infty - B_0)\n",
    "\n",
    "    # Exercise boundary\n",
    "    B = alpha * B_infty + (1 - alpha) * B_0\n",
    "\n",
    "    # If S ≥ B, immediate exercise\n",
    "    if S >= B:\n",
    "        return S - K\n",
    "\n",
    "    # Otherwise use approximation formula\n",
    "    lambda_val = -2 * b / (sigma ** 2)\n",
    "\n",
    "    d1 = (np.log(S / B) + (b + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "\n",
    "    part1 = (S * np.exp((b - r) * T) * norm.cdf(d1)) - (K * np.exp(-r * T) * norm.cdf(d1 - sigma * np.sqrt(T)))\n",
    "    part2 = (S * np.exp((b - r) * T) * norm.cdf(-d1)) - (K * np.exp(-r * T) * norm.cdf(-d1 + sigma * np.sqrt(T)))\n",
    "\n",
    "    return part1 - part2 * (S / B) ** lambda_val\n",
    "\n",
    "\n",
    "# 6. Monte Carlo simulation with LSM with parameters\n",
    "def monte_carlo_iv(row, r=DEFAULT_R, M=1000, N=50, max_vol=3.0, min_vol=0.001, max_iter=50, seed=42):\n",
    "    try:\n",
    "        S = row['current_underlying_mean_mid']\n",
    "        K = row['strike']\n",
    "        T = row['maturity'] / 365\n",
    "        price = row['mean_mid']\n",
    "        is_call = row['is_call'] == 1\n",
    "\n",
    "        # For efficiency, skip very deep ITM options\n",
    "        intrinsic = max(0, S - K) if is_call else max(0, K - S)\n",
    "        if intrinsic / price > 0.95:\n",
    "            return np.nan\n",
    "\n",
    "        def objective(sigma):\n",
    "            model_price = lsm_american_option_price(S, K, T, r, sigma, is_call, M=M, N=N,seed=seed)\n",
    "            return model_price - price\n",
    "\n",
    "        # Find the implied volatility with customizable parameters\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter)\n",
    "        return iv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def simulate_paths(S, r, sigma, T, M, N, seed=42):\n",
    "    \"\"\"Simulate asset price paths with optional seed parameter\"\"\"\n",
    "    dt = T / N\n",
    "    paths = np.zeros((M, N + 1))\n",
    "    paths[:, 0] = S\n",
    "\n",
    "    # Generate random paths with optional seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    Z = np.random.standard_normal((M, N))\n",
    "    for t in range(1, N + 1):\n",
    "        paths[:, t] = paths[:, t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt + sigma * np.sqrt(dt) * Z[:, t - 1])\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def lsm_american_option_price(S, K, T, r, sigma, is_call, M=1000, N=50, seed=42, min_itm_paths=5):\n",
    "    \"\"\"Price American option using Least Squares Monte Carlo method with parameters\"\"\"\n",
    "    # Simulate paths\n",
    "    paths = simulate_paths(S, r, sigma, T, M, N, seed=seed)\n",
    "    dt = T / N\n",
    "\n",
    "    # Initialize payoffs at maturity\n",
    "    if is_call:\n",
    "        payoffs = np.maximum(0, paths[:, -1] - K)\n",
    "    else:\n",
    "        payoffs = np.maximum(0, K - paths[:, -1])\n",
    "\n",
    "    # Working backwards through time\n",
    "    for t in range(N - 1, 0, -1):\n",
    "        # Identify in-the-money paths\n",
    "        if is_call:\n",
    "            itm = paths[:, t] > K\n",
    "        else:\n",
    "            itm = paths[:, t] < K\n",
    "\n",
    "        if sum(itm) > min_itm_paths:  # Need enough ITM paths for regression (now parameterized)\n",
    "            # Current stock prices for ITM options\n",
    "            S_itm = paths[itm, t]\n",
    "            # Future discounted cashflows for ITM options\n",
    "            V_itm = payoffs[itm] * np.exp(-r * dt * (N - t))\n",
    "            # Regression basis functions (polynomial)\n",
    "            X = np.column_stack([np.ones(len(S_itm)), S_itm, S_itm ** 2])\n",
    "            # Fit regression model\n",
    "            beta, _, _, _ = np.linalg.lstsq(X, V_itm, rcond=None)\n",
    "            # Expected continuation values\n",
    "            C = np.dot(X, beta)\n",
    "            # Immediate exercise values\n",
    "            if is_call:\n",
    "                exercise = np.maximum(0, S_itm - K)\n",
    "            else:\n",
    "                exercise = np.maximum(0, K - S_itm)\n",
    "\n",
    "            # Exercise decision\n",
    "            ex_idx = exercise > C\n",
    "\n",
    "            # Update payoffs\n",
    "            payoffs_new = np.copy(payoffs)\n",
    "            ex_path_idx = np.where(itm)[0][ex_idx]\n",
    "            payoffs_new[ex_path_idx] = exercise[ex_idx] * np.exp(-r * dt * t)\n",
    "\n",
    "            # Only consider unexercised paths for future iterations\n",
    "            payoffs = payoffs_new\n",
    "\n",
    "    # Option price is the average of all discounted payoffs\n",
    "    return np.mean(payoffs)\n",
    "\n",
    "def european_put_price(S, K, r, sigma, T):\n",
    "    \"\"\"Calculate European put price\"\"\"\n",
    "    if T <= 0 or sigma <= 0:\n",
    "        return max(K - S, 0)\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "\n",
    "\n",
    "def zhu_simplified_american_put_price(S, K, r, sigma, T, n_terms=15):\n",
    "    \"\"\"\n",
    "    Calculate American put price using simplified Zhu formula\n",
    "    For OTM puts, American = European approximately\n",
    "    \"\"\"\n",
    "\n",
    "    # ITM put: Apply early exercise premium\n",
    "    european_price = european_put_price(S, K, r, sigma, T)\n",
    "\n",
    "    # Early exercise premium (simplified approximation)\n",
    "    # In practice, this should use the full Zhu series expansion\n",
    "    gamma = 2 * r / (sigma ** 2)\n",
    "    tau = T * sigma ** 2 / 2\n",
    "\n",
    "    # Approximate early exercise premium\n",
    "    optimal_boundary_factor = np.exp(-gamma * tau)\n",
    "    early_premium = european_price * (1 - optimal_boundary_factor)\n",
    "\n",
    "    return european_price + early_premium\n",
    "\n",
    "\n",
    "def implied_vol_zhu_simplified(row, r=0.03, min_vol=0.001, max_vol=2.0, max_iter=100, debug=False):\n",
    "    \"\"\"Calculate implied volatility using Zhu's formula for American puts\"\"\"\n",
    "    S = row['current_underlying_mean_mid']\n",
    "    K = row['strike']\n",
    "    T = row['maturity'] / 365\n",
    "    price = row['mean_mid']\n",
    "    is_call = row['is_call'] == 1\n",
    "\n",
    "    # Skip call options or invalid data\n",
    "    if is_call or not all(np.isfinite([S, K, T, price])) or any(x <= 0 for x in [S, K, T, price]):\n",
    "        return np.nan\n",
    "\n",
    "    intrinsic = max(0, K - S)\n",
    "\n",
    "    # For OTM options, price should be less than strike\n",
    "    if S >= K and price > K:\n",
    "        print('impossible price')\n",
    "        return np.nan\n",
    "\n",
    "    # Price should be > intrinsic value\n",
    "    if price <= intrinsic * 1.001:\n",
    "        print('all most all intrinsic value')\n",
    "        return np.nan\n",
    "\n",
    "    def objective(sigma):\n",
    "        if sigma <= 1e-6 or sigma >= 10.0:\n",
    "            return float('inf')\n",
    "        try:\n",
    "            model_price = zhu_simplified_american_put_price(S, K, r, sigma, T)\n",
    "            if not np.isfinite(model_price):\n",
    "                return float('inf')\n",
    "            error = model_price - price\n",
    "\n",
    "            if debug:\n",
    "                euro_price = european_put_price(S, K, r, sigma, T)\n",
    "                print(f\"  sigma={sigma:.6f}: model={model_price:.6f}, euro={euro_price:.6f}, error={error:.6f}\")\n",
    "\n",
    "            return error\n",
    "        except Exception as e:\n",
    "            print(f\"  Error with sigma={sigma}: {str(e)}\")\n",
    "            return float('inf')\n",
    "\n",
    "    # First check if solutions exist at boundaries\n",
    "    f_min = objective(min_vol)\n",
    "    f_max = objective(max_vol)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"\\nIV calculation:\")\n",
    "        print(f\"  S={S:.4f}, K={K:.4f}, T={T:.6f}, price={price:.6f}\")\n",
    "        print(f\"  Intrinsic={intrinsic:.6f}\")\n",
    "        print(f\"  f(min_vol)={f_min:.6f}, f(max_vol)={f_max:.6f}\")\n",
    "\n",
    "    if not np.isfinite(f_min) or not np.isfinite(f_max) or f_min * f_max >= 0:\n",
    "        if debug:\n",
    "            print(f\"  No bracketing solution found\")\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        iv = brentq(objective, min_vol, max_vol, maxiter=max_iter, xtol=1e-8)\n",
    "\n",
    "\n",
    "        return iv\n",
    "    except Exception as e:\n",
    "        print(f\"  Exception: {str(e)}\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3821ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 option records...\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = 'sample_data.csv'  # Use your file path\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[df['is_call'] == 0]\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "# Ensure time_to_expiry column exists\n",
    "if 'time_to_expiry' not in df.columns:\n",
    "    df['time_to_expiry'] = df['maturity'] / 365.0\n",
    "\n",
    "# discount back to current price\n",
    "df['current_underlying_mean_mid'] = df['underly_mean_mid'] * np.exp(-DEFAULT_R * df['time_to_expiry'])\n",
    "# Use a subset of the data for faster testing\n",
    "sample_size = 10\n",
    "df_filtered = df[:sample_size]\n",
    "\n",
    "print(f\"Processing {sample_size} option records...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a9cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for binomial methods: 0.48 seconds\n",
      "Default valid results: 10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 1. Binomial Tree method with different parameters\n",
    "t0 = time.time()\n",
    "\n",
    "# Run with default parameters\n",
    "df_filtered['iv_binomial_default'] = df_filtered.apply(implied_vol_binomial, axis=1)\n",
    "\n",
    "# Run with custom parameters (example: more steps for better accuracy)\n",
    "# df_filtered['iv_binomial_middle_steps'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_binomial(row, steps=200), axis=1)\n",
    "\n",
    "# df_filtered['iv_binomial_high_steps'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_binomial(row, steps=500), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Run with different risk-free rate\n",
    "# df_filtered['iv_binomial_higher_r'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_binomial(row, r=0.04), axis=1)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Time spent for binomial methods: {t1 - t0:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_binomial_default\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'High steps valid results: {df_filtered[\"iv_binomial_high_steps\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher r valid results: {df_filtered[\"iv_binomial_higher_r\"].notna().sum()} out of {sample_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d361a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for quantlib binomial methods: 0.02 seconds\n",
      "Default valid results: 10 out of 10\n",
      "benchmark valid results: 10 out of 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # binomial tree using quantlib library\n",
    "df_filtered['iv_benchmark'] = df_filtered.apply(\n",
    "lambda row: quantlib_binomial_spot_iv(row, steps=5000), axis=1)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "df_filtered['iv_ql_binomial_default'] = df_filtered.apply(quantlib_binomial_spot_iv, axis=1)\n",
    "# df_filtered['iv_ql_binomial_middle_steps'] = df_filtered.apply(\n",
    "#     lambda row: quantlib_binomial_spot_iv(row, steps=300), axis=1)\n",
    "# df_filtered['iv_ql_binomial_high_steps'] = df_filtered.apply(\n",
    "#     lambda row: quantlib_binomial_spot_iv(row, steps=500), axis=1)\n",
    "# df_filtered['iv_ql_binomial_highest_steps'] = df_filtered.apply(\n",
    "#     lambda row: quantlib_binomial_spot_iv(row, steps=1000), axis=1)\n",
    "\n",
    "\n",
    "# df_filtered['iv_ql_binomial_more_iters_steps'] = df_filtered.apply(\n",
    "#     lambda row: quantlib_binomial_spot_iv(row, max_iter=200), axis=1)\n",
    "\n",
    "t3 = time.time()\n",
    "print(f'Time spent for quantlib binomial methods: {t3 - t2:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_ql_binomial_default\"].notna().sum()} out of {sample_size}')\n",
    "print(f'benchmark valid results: {df_filtered[\"iv_benchmark\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'High steps valid results: {df_filtered[\"iv_ql_binomial_high_steps\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher r iters results: {df_filtered[\"iv_ql_binomial_more_iters_steps\"].notna().sum()} out of {sample_size}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f43a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for Trinomial Tree methods: 1.12 seconds\n",
      "Default valid results: 10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 2. Trinomial Tree method with different parameters\n",
    "t4 = time.time()\n",
    "\n",
    "# Run with default parameters\n",
    "# df_filtered['iv_trinomial_default'] = df_filtered.apply(implied_vol_trinomial, axis=1)\n",
    "df_filtered['iv_trinomial_default'] = df_filtered.apply(implied_vol_trinomial, axis=1)\n",
    "\n",
    "# Run with more steps for better accuracy\n",
    "# df_filtered['iv_trinomial_middle_steps'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_trinomial(row, steps=300), axis=1)\n",
    "\n",
    "# df_filtered['iv_trinomial_high_steps'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_trinomial(row, steps=500), axis=1)\n",
    "\n",
    "\n",
    "# df_filtered['iv_ql_trinomial_more_iters_steps'] = df_filtered.apply(\n",
    "#     lambda row: implied_vol_trinomial(row, max_iter=200), axis=1)\n",
    "\n",
    "t5 = time.time()\n",
    "print(f'Time spent for Trinomial Tree methods: {t5 - t4:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_trinomial_default\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Middle steps valid results: {df_filtered[\"iv_trinomial_middle_steps\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Most steps valid results: {df_filtered[\"iv_trinomial_high_steps\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher iters_steps valid results: {df_filtered[\"iv_ql_trinomial_more_iters_steps\"].notna().sum()} out of {sample_size}')\n",
    "\n",
    "    #trinomial tree does not have popular package can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52bb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9e2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for BAW methods: 0.02 seconds\n",
      "Default valid results: 10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 3. Barone-Adesi and Whaley (BAW) method with different parameers\n",
    "t6 = time.time()\n",
    "\n",
    "# Run with default parameters\n",
    "df_filtered['iv_baw_default'] = df_filtered.apply(baw_american_iv, axis=1)\n",
    "\n",
    "\n",
    "# Run with higher max_iter\n",
    "# df_filtered['iv_baw_higher_iter'] = df_filtered.apply(\n",
    "#     lambda row: baw_american_iv(row, max_iter=200), axis=1)\n",
    "\n",
    "\n",
    "# df_filtered['iv_baw_higher_r'] = df_filtered.apply(\n",
    "#     lambda row: baw_american_iv(row,  r=0.04), axis=1)\n",
    "\n",
    "# more iteration make almost no difference\n",
    "# is more sensitive to r than iteration\n",
    "\n",
    "t7 = time.time()\n",
    "print(f'Time spent for BAW methods: {t7 - t6:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_baw_default\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher iter valid results: {df_filtered[\"iv_baw_higher_iter\"].notna().sum()} out of {sample_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab5e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for Bjerksund-Stensland methods: 0.00 seconds\n",
      "Default valid results: 0 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 4. Bjerksund-Stensland method with different parameters\n",
    "t8 = time.time()\n",
    "\n",
    "# Run with default parameters\n",
    "df_filtered['iv_bjerksund_default'] = df_filtered.apply(bjerksund_iv, axis=1)\n",
    "\n",
    "# df_filtered['iv_bjerksund_more_iteration'] =  df_filtered.apply(\n",
    "#     lambda row: bjerksund_iv(row, max_iter=500), axis=1)\n",
    "\n",
    "# # Run with different min/max volatility bounds\n",
    "# df_filtered['iv_bjerksund_wider_bounds'] = df_filtered.apply(\n",
    "#     lambda row: bjerksund_iv(row, min_vol=0.0001, max_vol=5.0), axis=1)\n",
    "\n",
    "# # Run with different min/max volatility bounds\n",
    "# df_filtered['iv_bjerksund_smaller_bounds'] = df_filtered.apply(\n",
    "#     lambda row: bjerksund_iv(row, min_vol=0.1, max_vol=0.5), axis=1)\n",
    "\n",
    "# # Run with different risk-free rate and dividend yield\n",
    "# df_filtered['iv_bjerksund_diff_r'] = df_filtered.apply(\n",
    "#     lambda row: bjerksund_iv(row, r=0.02), axis=1)\n",
    "\n",
    "t9 = time.time()\n",
    "#put 都算不出来 算不出来的还是算不出出来，wider bound 和 more iteration基本没用，r影响比其他都大\n",
    "\n",
    "print(f'Time spent for Bjerksund-Stensland methods: {t9 - t8:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_bjerksund_default\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'More Iteration valid results: {df_filtered[\"iv_bjerksund_more_iteration\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Wider bounds valid results: {df_filtered[\"iv_bjerksund_wider_bounds\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Smaller bounds valid results: {df_filtered[\"iv_bjerksund_smaller_bounds\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Different r valid results: {df_filtered[\"iv_bjerksund_diff_r\"].notna().sum()} out of {sample_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for European approximation methods: 0.01 seconds\n",
      "Default valid results: 10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 5. Black-Scholes European approximation with different parameters\n",
    "t10 = time.time()\n",
    "\n",
    "# Run with default parameters\n",
    "df_filtered['iv_euro_default'] = df_filtered.apply(compute_iv_euro, axis=1)\n",
    "\n",
    "# # Run with higher risk-free rate\n",
    "# df_filtered['iv_euro_higher_r'] = df_filtered.apply(\n",
    "#     lambda row: compute_iv_euro(row, r=0.04), axis=1)\n",
    "\n",
    "# # Run with different max iterations\n",
    "# df_filtered['iv_euro_lower_iter'] = df_filtered.apply(\n",
    "#     lambda row: compute_iv_euro(row, max_iter=50), axis=1)\n",
    "\n",
    "# # Run with different max iterations\n",
    "# df_filtered['iv_euro_higher_iter'] = df_filtered.apply(\n",
    "#     lambda row: compute_iv_euro(row, max_iter=300), axis=1)\n",
    "\n",
    "t11 = time.time()\n",
    "print(f'Time spent for European approximation methods: {t11 - t10:.2f} seconds')\n",
    "print(f'Default valid results: {df_filtered[\"iv_euro_default\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher r valid results: {df_filtered[\"iv_euro_higher_r\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Lower iter valid results: {df_filtered[\"iv_euro_lower_iter\"].notna().sum()} out of {sample_size}')\n",
    "# print(f'Higher iter valid results: {df_filtered[\"iv_euro_higher_iter\"].notna().sum()} out of {sample_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7334acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for Monte Carlo methods: 1.10 seconds\n",
      "Default valid results: 10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# 6. Monte Carlo method with different parameters (more time-consuming)\n",
    "t12 = time.time()\n",
    "\n",
    "df_filtered['iv_monte_carlo_default'] = df_filtered.apply(monte_carlo_iv, axis=1)\n",
    "\n",
    "# Run with different seed for different random numbers\n",
    "# df_filtered['iv_monte_carlo_diff_seed'] = df_filtered.apply(\n",
    "#     lambda row: monte_carlo_iv(row, seed=123), axis=1)\n",
    "\n",
    "t13 = time.time()\n",
    "print(f'Time spent for Monte Carlo methods: {t13 - t12:.2f} seconds')\n",
    "\n",
    "\n",
    "# t14 = time.time()\n",
    "\n",
    "# Run with more paths for better accuracy (but slower)\n",
    "# df_filtered['iv_monte_carlo_more_paths'] = df_filtered.apply(\n",
    "#     lambda row: monte_carlo_iv(row, M=2000, N=100), axis=1)\n",
    "\n",
    "# t15 = time.time()\n",
    "# print(f'Time spent for Monte Carlo methods: {t15 - t14:.2f} seconds')\n",
    "\n",
    "print(\n",
    "    f'Default valid results: {df_filtered[\"iv_monte_carlo_default\"].notna().sum()} out of {len(df_filtered)}')\n",
    "# print(\n",
    "#     f'More paths valid results: {df_filtered[\"iv_monte_carlo_more_paths\"].notna().sum()} out of {len(df_filtered)}')\n",
    "# print(\n",
    "#     f'Different seed valid results: {df_filtered[\"iv_monte_carlo_diff_seed\"].notna().sum()} out of {len(df_filtered)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e264cd4fefc34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t16 = time.time()\n",
    "# # Run with more paths for best accuracy (but slower)\n",
    "# df_filtered['iv_monte_carlo_most_paths'] = df_filtered.apply(\n",
    "#     lambda row: monte_carlo_iv(row, M=4000, N=200), axis=1)\n",
    "\n",
    "# df_filtered['iv_monte_carlo_most_paths_other_seed'] = df_filtered.apply(\n",
    "#     lambda row: monte_carlo_iv(row, M=4000, N=200, seed=123), axis=1)\n",
    "\n",
    "# t17 = time.time()\n",
    "# print(f'Time spent for Monte Carlo methods: {t17 - t16:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7496479a4b4f579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e311d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for zhu methods: 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. simplified zhu's method\n",
    "\n",
    "t18 = time.time()\n",
    "\n",
    "df_filtered['iv_zhu'] = df_filtered.apply(\n",
    "    lambda row: implied_vol_zhu_simplified(row), axis=1)\n",
    "\n",
    "t19 = time.time()\n",
    "\n",
    "print(f'Time spent for zhu methods: {t19 - t18:.2f} seconds')\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27cb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b4a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2803b8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for zhu methods: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. simplified zhu's method\n",
    "\n",
    "t18 = time.time()\n",
    "\n",
    "df_filtered['iv_zhu'] = df_filtered.apply(\n",
    "    lambda row: implied_vol_zhu_simplified(row), axis=1)\n",
    "\n",
    "t19 = time.time()\n",
    "\n",
    "print(f'Time spent for zhu methods: {t19 - t18:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d684d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54de8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>mean_ask</th>\n",
       "      <th>mean_bid</th>\n",
       "      <th>cumcashvol</th>\n",
       "      <th>cumvolume</th>\n",
       "      <th>openinterest</th>\n",
       "      <th>is_call</th>\n",
       "      <th>strike</th>\n",
       "      <th>...</th>\n",
       "      <th>current_underlying_mean_mid</th>\n",
       "      <th>iv_binomial_default</th>\n",
       "      <th>iv_benchmark</th>\n",
       "      <th>iv_ql_binomial_default</th>\n",
       "      <th>iv_trinomial_default</th>\n",
       "      <th>iv_baw_default</th>\n",
       "      <th>iv_bjerksund_default</th>\n",
       "      <th>iv_euro_default</th>\n",
       "      <th>iv_monte_carlo_default</th>\n",
       "      <th>iv_zhu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P13800</td>\n",
       "      <td>84.250000</td>\n",
       "      <td>79.625000</td>\n",
       "      <td>5295.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3955.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14889.711256</td>\n",
       "      <td>0.175899</td>\n",
       "      <td>0.175654</td>\n",
       "      <td>0.175845</td>\n",
       "      <td>0.175862</td>\n",
       "      <td>0.056804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176227</td>\n",
       "      <td>0.168773</td>\n",
       "      <td>0.175898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P14200</td>\n",
       "      <td>142.631579</td>\n",
       "      <td>138.578947</td>\n",
       "      <td>12745.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14896.285167</td>\n",
       "      <td>0.164526</td>\n",
       "      <td>0.164773</td>\n",
       "      <td>0.164635</td>\n",
       "      <td>0.164654</td>\n",
       "      <td>0.057061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.165688</td>\n",
       "      <td>0.160572</td>\n",
       "      <td>0.165271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P14400</td>\n",
       "      <td>189.666667</td>\n",
       "      <td>184.888889</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5919.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14890.470763</td>\n",
       "      <td>0.159980</td>\n",
       "      <td>0.160323</td>\n",
       "      <td>0.160155</td>\n",
       "      <td>0.160175</td>\n",
       "      <td>0.057575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.161516</td>\n",
       "      <td>0.157307</td>\n",
       "      <td>0.161027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P14600</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>250.500000</td>\n",
       "      <td>25100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7909.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14886.604184</td>\n",
       "      <td>0.157727</td>\n",
       "      <td>0.158099</td>\n",
       "      <td>0.158210</td>\n",
       "      <td>0.158236</td>\n",
       "      <td>0.058950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.159709</td>\n",
       "      <td>0.156263</td>\n",
       "      <td>0.159113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P14800</td>\n",
       "      <td>326.272727</td>\n",
       "      <td>320.272727</td>\n",
       "      <td>21100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14888.411935</td>\n",
       "      <td>0.153680</td>\n",
       "      <td>0.154067</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>0.153907</td>\n",
       "      <td>0.060766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.153403</td>\n",
       "      <td>0.155546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>2024-06-03 21:00</td>\n",
       "      <td>CF2409P15000</td>\n",
       "      <td>421.071429</td>\n",
       "      <td>406.928571</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14891.575499</td>\n",
       "      <td>0.151237</td>\n",
       "      <td>0.151619</td>\n",
       "      <td>0.151531</td>\n",
       "      <td>0.151561</td>\n",
       "      <td>0.064132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.154704</td>\n",
       "      <td>0.147725</td>\n",
       "      <td>0.153796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>2024-06-03 21:01</td>\n",
       "      <td>CF2409P13400</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>48.733333</td>\n",
       "      <td>7695.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14891.244078</td>\n",
       "      <td>0.189580</td>\n",
       "      <td>0.189736</td>\n",
       "      <td>0.189979</td>\n",
       "      <td>0.189995</td>\n",
       "      <td>0.057194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>0.180348</td>\n",
       "      <td>0.189852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>2024-06-03 21:01</td>\n",
       "      <td>CF2409P13800</td>\n",
       "      <td>83.032258</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>5705.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3956.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14891.655682</td>\n",
       "      <td>0.176145</td>\n",
       "      <td>0.175896</td>\n",
       "      <td>0.176090</td>\n",
       "      <td>0.176108</td>\n",
       "      <td>0.056840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176468</td>\n",
       "      <td>0.169006</td>\n",
       "      <td>0.176139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>2024-06-03 21:01</td>\n",
       "      <td>CF2409P14000</td>\n",
       "      <td>107.161290</td>\n",
       "      <td>105.354839</td>\n",
       "      <td>27030.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>10167.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14891.575499</td>\n",
       "      <td>0.169299</td>\n",
       "      <td>0.169245</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.169311</td>\n",
       "      <td>0.056737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>0.169603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>2024-06-03 21:01</td>\n",
       "      <td>CF2409P14200</td>\n",
       "      <td>144.983607</td>\n",
       "      <td>141.491803</td>\n",
       "      <td>16370.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14891.371757</td>\n",
       "      <td>0.165371</td>\n",
       "      <td>0.165573</td>\n",
       "      <td>0.165402</td>\n",
       "      <td>0.165421</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166490</td>\n",
       "      <td>0.160978</td>\n",
       "      <td>0.166068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        minute_str    Instrument    mean_ask    mean_bid  \\\n",
       "0           0  2024-06-03 21:00  CF2409P13800   84.250000   79.625000   \n",
       "1           1  2024-06-03 21:00  CF2409P14200  142.631579  138.578947   \n",
       "2           2  2024-06-03 21:00  CF2409P14400  189.666667  184.888889   \n",
       "3           3  2024-06-03 21:00  CF2409P14600  252.000000  250.500000   \n",
       "4           5  2024-06-03 21:00  CF2409P14800  326.272727  320.272727   \n",
       "5           7  2024-06-03 21:00  CF2409P15000  421.071429  406.928571   \n",
       "6          12  2024-06-03 21:01  CF2409P13400   50.000000   48.733333   \n",
       "7          13  2024-06-03 21:01  CF2409P13800   83.032258   81.000000   \n",
       "8          14  2024-06-03 21:01  CF2409P14000  107.161290  105.354839   \n",
       "9          15  2024-06-03 21:01  CF2409P14200  144.983607  141.491803   \n",
       "\n",
       "   cumcashvol  cumvolume  openinterest  is_call   strike  ...  \\\n",
       "0      5295.0       13.0        3955.0      0.0  13800.0  ...   \n",
       "1     12745.0       18.0           0.0      0.0  14200.0  ...   \n",
       "2      7500.0        8.0        5919.0      0.0  14400.0  ...   \n",
       "3     25100.0       20.0        7909.0      0.0  14600.0  ...   \n",
       "4     21100.0       13.0           0.0      0.0  14800.0  ...   \n",
       "5      2000.0        1.0           0.0      0.0  15000.0  ...   \n",
       "6      7695.0       31.0        7920.0      0.0  13400.0  ...   \n",
       "7      5705.0       14.0        3956.0      0.0  13800.0  ...   \n",
       "8     27030.0       51.0       10167.0      0.0  14000.0  ...   \n",
       "9     16370.0       23.0       11447.0      0.0  14200.0  ...   \n",
       "\n",
       "   current_underlying_mean_mid  iv_binomial_default  iv_benchmark  \\\n",
       "0                 14889.711256             0.175899      0.175654   \n",
       "1                 14896.285167             0.164526      0.164773   \n",
       "2                 14890.470763             0.159980      0.160323   \n",
       "3                 14886.604184             0.157727      0.158099   \n",
       "4                 14888.411935             0.153680      0.154067   \n",
       "5                 14891.575499             0.151237      0.151619   \n",
       "6                 14891.244078             0.189580      0.189736   \n",
       "7                 14891.655682             0.176145      0.175896   \n",
       "8                 14891.575499             0.169299      0.169245   \n",
       "9                 14891.371757             0.165371      0.165573   \n",
       "\n",
       "   iv_ql_binomial_default iv_trinomial_default  iv_baw_default  \\\n",
       "0                0.175845             0.175862        0.056804   \n",
       "1                0.164635             0.164654        0.057061   \n",
       "2                0.160155             0.160175        0.057575   \n",
       "3                0.158210             0.158236        0.058950   \n",
       "4                0.153879             0.153907        0.060766   \n",
       "5                0.151531             0.151561        0.064132   \n",
       "6                0.189979             0.189995        0.057194   \n",
       "7                0.176090             0.176108        0.056840   \n",
       "8                0.169292             0.169311        0.056737   \n",
       "9                0.165402             0.165421        0.057246   \n",
       "\n",
       "   iv_bjerksund_default  iv_euro_default  iv_monte_carlo_default    iv_zhu  \n",
       "0                   NaN         0.176227                0.168773  0.175898  \n",
       "1                   NaN         0.165688                0.160572  0.165271  \n",
       "2                   NaN         0.161516                0.157307  0.161027  \n",
       "3                   NaN         0.159709                0.156263  0.159113  \n",
       "4                   NaN         0.156271                0.153403  0.155546  \n",
       "5                   NaN         0.154704                0.147725  0.153796  \n",
       "6                   NaN         0.190132                0.180348  0.189852  \n",
       "7                   NaN         0.176468                0.169006  0.176139  \n",
       "8                   NaN         0.169967                0.162856  0.169603  \n",
       "9                   NaN         0.166490                0.160978  0.166068  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af357c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46705c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55700a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af30e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "def analyze_iv_results(df_filtered, benchmark_col='iv_benchmark'):\n",
    "    \"\"\"\n",
    "    Calculate error metrics for each IV method compared to the benchmark.\n",
    "    Treats NaN values as zeros for error calculation.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered: DataFrame containing IV calculation results\n",
    "        benchmark_col: Column name for the benchmark IV values\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with error metrics\n",
    "    \"\"\"\n",
    "    # Get all IV columns except benchmark\n",
    "    iv_cols = [col for col in df_filtered.columns if col.startswith('iv_') and col != benchmark_col]\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame(index=iv_cols)\n",
    "    \n",
    "    # Calculate error metrics for each method\n",
    "    for col in iv_cols:\n",
    "        # For rows where benchmark has valid values\n",
    "        benchmark_valid = df_filtered[benchmark_col].notna()\n",
    "        \n",
    "        if benchmark_valid.sum() > 0:\n",
    "            # Create a copy of the data with NaN values filled with zeros\n",
    "            filled_df = df_filtered.copy()\n",
    "            filled_df[col].fillna(0, inplace=True)\n",
    "            filled_df[benchmark_col].fillna(0, inplace=True)\n",
    "            \n",
    "            # Calculate errors where benchmark is valid\n",
    "            errors = filled_df.loc[benchmark_valid, col] - filled_df.loc[benchmark_valid, benchmark_col]\n",
    "            abs_errors = np.abs(errors)\n",
    "            # Avoid division by zero in percentage calculation\n",
    "            percent_errors = abs_errors / np.maximum(filled_df.loc[benchmark_valid, benchmark_col], 1e-10) * 100\n",
    "            \n",
    "            # Calculate how many values were actually valid in the method\n",
    "            valid_count = df_filtered.loc[benchmark_valid, col].notna().sum()\n",
    "            \n",
    "            # Store metrics\n",
    "            results.loc[col, 'Total Count'] = benchmark_valid.sum()\n",
    "            results.loc[col, 'Valid Count'] = valid_count\n",
    "            results.loc[col, 'Valid Percentage'] = valid_count / benchmark_valid.sum() * 100\n",
    "            results.loc[col, 'Mean Absolute Error'] = abs_errors.mean()\n",
    "            results.loc[col, 'RMSE'] = np.sqrt((errors**2).mean())\n",
    "            results.loc[col, 'Mean % Error'] = percent_errors.mean()\n",
    "            results.loc[col, 'Max Absolute Error'] = abs_errors.max()\n",
    "        else:\n",
    "            # No valid benchmark values\n",
    "            results.loc[col, 'Total Count'] = 0\n",
    "            results.loc[col, 'Valid Count'] = 0\n",
    "            results.loc[col, 'Valid Percentage'] = 0\n",
    "            results.loc[col, 'Mean Absolute Error'] = np.nan\n",
    "            results.loc[col, 'RMSE'] = np.nan\n",
    "            results.loc[col, 'Mean % Error'] = np.nan\n",
    "            results.loc[col, 'Max Absolute Error'] = np.nan\n",
    "    \n",
    "    # Sort by RMSE (lower is better)\n",
    "    return results.sort_values('RMSE')\n",
    "\n",
    "def plot_iv_comparison(df_filtered, benchmark_col='iv_benchmark', output_file='iv_comparison.png'):\n",
    "    \"\"\"\n",
    "    Create scatter plots comparing each method to the benchmark.\n",
    "    Fills NaN values with zeros for visualization.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered: DataFrame containing IV calculation results\n",
    "        benchmark_col: Column name for the benchmark IV values\n",
    "        output_file: Filename to save the plot\n",
    "    \"\"\"\n",
    "    # Get all IV columns except benchmark and time columns\n",
    "    iv_cols = [col for col in df_filtered.columns if col.startswith('iv_') and \n",
    "               col != benchmark_col and not col.endswith('_time')]\n",
    "    \n",
    "    # Create a filled version of the DataFrame\n",
    "    df_filled = df_filtered.copy()\n",
    "    for col in iv_cols + [benchmark_col]:\n",
    "        df_filled[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Determine plot layout\n",
    "    n_methods = len(iv_cols)\n",
    "    n_cols = min(3, n_methods)\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows * n_cols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Create scatter plots for each method\n",
    "    for i, col in enumerate(iv_cols):\n",
    "        if i < len(axes):\n",
    "            # Get data\n",
    "            x = df_filled[benchmark_col]\n",
    "            y = df_filled[col]\n",
    "            \n",
    "            # Create scatter plot\n",
    "            axes[i].scatter(x, y, alpha=0.7)\n",
    "            \n",
    "            # Add identity line\n",
    "            min_val = min(x.min(), y.min())\n",
    "            max_val = max(x.max(), y.max())\n",
    "            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            \n",
    "            # Add labels\n",
    "            axes[i].set_xlabel(f'{benchmark_col} IV')\n",
    "            axes[i].set_ylabel(f'{col} IV')\n",
    "            \n",
    "            # Calculate error metrics for title\n",
    "            errors = y - x\n",
    "            mae = np.abs(errors).mean()\n",
    "            rmse = np.sqrt((errors**2).mean())\n",
    "            \n",
    "            # Set title\n",
    "            method_name = col.replace('iv_', '').replace('_', ' ').title()\n",
    "            axes[i].set_title(f'{method_name} vs Benchmark\\nMAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
    "            \n",
    "            # Highlight NaN values with a different color\n",
    "            nan_mask = df_filtered[col].isna()\n",
    "            if nan_mask.any():\n",
    "                axes[i].scatter(df_filled.loc[nan_mask, benchmark_col], \n",
    "                               df_filled.loc[nan_mask, col], \n",
    "                               color='red', alpha=0.3, \n",
    "                               label='NaN (set to 0)')\n",
    "                axes[i].legend()\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for i in range(n_methods, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_error_boxplot(df_filtered, benchmark_col='iv_benchmark', output_file='iv_error_boxplot.png'):\n",
    "    \"\"\"\n",
    "    Create a boxplot of absolute errors for each method.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered: DataFrame containing IV calculation results\n",
    "        benchmark_col: Column name for the benchmark IV values\n",
    "        output_file: Filename to save the plot\n",
    "    \"\"\"\n",
    "    # Get all IV columns except benchmark\n",
    "    iv_cols = [col for col in df_filtered.columns if col.startswith('iv_') and \n",
    "               col != benchmark_col and not col.endswith('_time')]\n",
    "    \n",
    "    # Create a filled version of the DataFrame\n",
    "    df_filled = df_filtered.copy()\n",
    "    for col in iv_cols + [benchmark_col]:\n",
    "        df_filled[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Create error DataFrame\n",
    "    error_df = pd.DataFrame()\n",
    "    for col in iv_cols:\n",
    "        method_name = col.replace('iv_', '').replace('_', ' ').title()\n",
    "        error_df[method_name] = np.abs(df_filled[col] - df_filled[benchmark_col])\n",
    "    \n",
    "    # Create boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(data=error_df)\n",
    "    plt.title('Absolute Errors Compared to Benchmark')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.xlabel('Method')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def run_iv_error_analysis(df_filtered):\n",
    "    \"\"\"\n",
    "    Run a complete error analysis on IV calculation methods.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered: DataFrame containing IV calculation results\n",
    "    \"\"\"\n",
    "    # Make sure we have a benchmark column\n",
    "    benchmark_col = 'iv_benchmark'\n",
    "    if benchmark_col not in df_filtered.columns:\n",
    "        raise ValueError(f\"Benchmark column '{benchmark_col}' not found in DataFrame\")\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    print(\"Calculating error metrics...\")\n",
    "    error_results = analyze_iv_results(df_filtered, benchmark_col)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\nError metrics for each IV method compared to benchmark:\")\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    pd.set_option('display.width', 150)  # Wider display\n",
    "    print(error_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    error_results.to_csv('iv_error_results.csv')\n",
    "    \n",
    "    # Print the best methods\n",
    "    print(\"\\nTop 3 methods by RMSE:\")\n",
    "    top_methods = error_results.head(3).index\n",
    "    for i, method in enumerate(top_methods):\n",
    "        print(f\"{i+1}. {method}\")\n",
    "        print(f\"   RMSE: {error_results.loc[method, 'RMSE']:.6f}\")\n",
    "        print(f\"   Mean Absolute Error: {error_results.loc[method, 'Mean Absolute Error']:.6f}\")\n",
    "        print(f\"   Mean % Error: {error_results.loc[method, 'Mean % Error']:.2f}%\")\n",
    "        print(f\"   Valid %: {error_results.loc[method, 'Valid Percentage']:.1f}%\")\n",
    "    \n",
    "    # Plot comparisons\n",
    "    print(\"\\nCreating comparison plots...\")\n",
    "    try:\n",
    "        plot_iv_comparison(df_filtered, benchmark_col)\n",
    "        print(\"Created scatter plot comparison: iv_comparison.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating scatter plots: {e}\")\n",
    "    \n",
    "    try:\n",
    "        create_error_boxplot(df_filtered, benchmark_col)\n",
    "        print(\"Created error boxplot: iv_error_boxplot.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating boxplot: {e}\")\n",
    "    \n",
    "    # Compare all to benchmark with filled NaNs\n",
    "    df_results = df_filtered.copy()\n",
    "    iv_cols = [col for col in df_results.columns if col.startswith('iv_') and not col.endswith('_time')]\n",
    "    \n",
    "    # Create dataframe for comparison\n",
    "    comparison_df = pd.DataFrame()\n",
    "    for col in iv_cols:\n",
    "        comparison_df[col] = df_results[col]\n",
    "    \n",
    "    # Replace NaN with 0 and calculate differences\n",
    "    comparison_df_filled = comparison_df.fillna(0)\n",
    "    for col in [c for c in iv_cols if c != benchmark_col]:\n",
    "        comparison_df_filled[f'{col}_diff'] = comparison_df_filled[col] - comparison_df_filled[benchmark_col]\n",
    "    \n",
    "    # Save detailed results\n",
    "    comparison_df.to_csv('iv_values_comparison.csv')\n",
    "    comparison_df_filled.to_csv('iv_values_comparison_filled.csv')\n",
    "    \n",
    "    # Print success rate for each method\n",
    "    print(\"\\nSuccess rate for each method:\")\n",
    "    for col in [c for c in iv_cols if c != benchmark_col]:\n",
    "        success_rate = (df_filtered[col].notna().sum() / len(df_filtered)) * 100\n",
    "        print(f\"{col}: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to CSV files.\")\n",
    "    return error_results\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a8382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd86b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'minute_str', 'Instrument', 'mean_ask', 'mean_bid',\n",
      "       'cumcashvol', 'cumvolume', 'openinterest', 'is_call', 'strike',\n",
      "       'maturity', 'underly_mean_mid', 'mean_mid', 'twap', 'date_str',\n",
      "       'time_to_expiry', 'current_underlying_mean_mid', 'iv_binomial_default',\n",
      "       'iv_benchmark', 'iv_ql_binomial_default', 'iv_trinomial_default',\n",
      "       'iv_baw_default', 'iv_bjerksund_default', 'iv_euro_default',\n",
      "       'iv_monte_carlo_default', 'iv_zhu'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0de8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'minute_str', 'Instrument', 'mean_ask', 'mean_bid',\n",
      "       'cumcashvol', 'cumvolume', 'openinterest', 'is_call', 'strike',\n",
      "       'maturity', 'underly_mean_mid', 'mean_mid', 'twap', 'date_str',\n",
      "       'time_to_expiry', 'current_underlying_mean_mid', 'iv_binomial_default',\n",
      "       'iv_benchmark', 'iv_ql_binomial_default', 'iv_trinomial_default',\n",
      "       'iv_baw_default', 'iv_bjerksund_default', 'iv_euro_default',\n",
      "       'iv_monte_carlo_default', 'iv_zhu'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb3bd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=df_filtered[['iv_binomial_default', 'iv_benchmark',\n",
    "       'iv_trinomial_default', 'iv_baw_default', 'iv_bjerksund_default', 'iv_euro_default', 'iv_monte_carlo_default', 'iv_zhu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ffd6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating error metrics...\n",
      "\n",
      "Error metrics for each IV method compared to benchmark:\n",
      "                        Total Count  Valid Count  Valid Percentage  Mean Absolute Error      RMSE  Mean % Error  Max Absolute Error\n",
      "iv_trinomial_default           10.0         10.0             100.0             0.000152  0.000163      0.089925            0.000258\n",
      "iv_binomial_default            10.0         10.0             100.0             0.000264  0.000284      0.162061            0.000387\n",
      "iv_zhu                         10.0         10.0             100.0             0.000733  0.000959      0.462712            0.002177\n",
      "iv_euro_default                10.0         10.0             100.0             0.001218  0.001463      0.762219            0.003085\n",
      "iv_monte_carlo_default         10.0         10.0             100.0             0.004776  0.005391      2.792484            0.009388\n",
      "iv_baw_default                 10.0         10.0             100.0             0.108168  0.108908     64.753666            0.132543\n",
      "iv_bjerksund_default           10.0          0.0               0.0             0.166499  0.166861    100.000000            0.189736\n",
      "\n",
      "Top 3 methods by RMSE:\n",
      "1. iv_trinomial_default\n",
      "   RMSE: 0.000163\n",
      "   Mean Absolute Error: 0.000152\n",
      "   Mean % Error: 0.09%\n",
      "   Valid %: 100.0%\n",
      "2. iv_binomial_default\n",
      "   RMSE: 0.000284\n",
      "   Mean Absolute Error: 0.000264\n",
      "   Mean % Error: 0.16%\n",
      "   Valid %: 100.0%\n",
      "3. iv_zhu\n",
      "   RMSE: 0.000959\n",
      "   Mean Absolute Error: 0.000733\n",
      "   Mean % Error: 0.46%\n",
      "   Valid %: 100.0%\n",
      "\n",
      "Creating comparison plots...\n",
      "Created scatter plot comparison: iv_comparison.png\n",
      "Created error boxplot: iv_error_boxplot.png\n",
      "\n",
      "Success rate for each method:\n",
      "iv_binomial_default: 100.0%\n",
      "iv_trinomial_default: 100.0%\n",
      "iv_baw_default: 100.0%\n",
      "iv_bjerksund_default: 0.0%\n",
      "iv_euro_default: 100.0%\n",
      "iv_monte_carlo_default: 100.0%\n",
      "iv_zhu: 100.0%\n",
      "\n",
      "Analysis complete! Results saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "error_results = run_iv_error_analysis(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98573df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
